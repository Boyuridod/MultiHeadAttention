# Lembrete

Vale lembrar que todos os vídeos as pessoas estão ensinando que inputEmbedding e PositionalEncoding tem [n] colunas e [dmodel] linhas,
sendo que na verdade estas matrizes tem [n] linhas e [dmodel] colunas. Porque?
Porque na verdade estas matrizes são [n] [vetores] cada um com [dmodel] elementos

# Referências

1. VASWANI, Ashish et al. *Attention Is All You Need*, 2023. Acesso em: 7 fev. 2025.
2. [Explicação completa do modelo Transformer com base no paper: Attention Is All You Need](https://youtu.be/aCWm4eMQlQs)
3. [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://youtu.be/bCz4OMemCcA)